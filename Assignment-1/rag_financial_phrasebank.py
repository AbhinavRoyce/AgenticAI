# -*- coding: utf-8 -*-
"""RAG_Financial_PhraseBank.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZD89a5A2XswIp_pGVAeYGpuzXg6ebAmP

# RAG Implementation using Financial PhraseBank Dataset
This notebook implements a Retrieval-Augmented Generation (RAG) pipeline
using the HuggingFace Financial PhraseBank dataset.
"""

!pip install datasets sentence-transformers faiss-cpu transformers -q

!pip uninstall -y datasets
!pip install datasets==2.18.0

from datasets import load_dataset

dataset = load_dataset("financial_phrasebank", "sentences_allagree", split="train[:500]")

documents = [item['sentence'] for item in dataset]

print("Total documents:", len(documents))
print(documents[:5])

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

embeddings = model.encode(documents)

print("Embedding shape:", embeddings.shape)

import faiss
import numpy as np

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)

index.add(np.array(embeddings))

print("Total vectors in index:", index.ntotal)

def retrieve(query, k=3):
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding), k)
    return [documents[i] for i in indices[0]]

query = "Why did the company profit increase?"
results = retrieve(query)

print("Query:", query)
print("\nRetrieved Documents:")
for doc in results:
    print("-", doc)

from transformers import pipeline

generator = pipeline("text-generation", model="gpt2")

def generate_answer(query):
    context = " ".join(retrieve(query))
    prompt = f"Context: {context}\n\nQuestion: {query}\nAnswer:"
    response = generator(prompt, max_length=200, num_return_sequences=1)
    return response[0]['generated_text']

print(generate_answer("What affected company earnings?"))

